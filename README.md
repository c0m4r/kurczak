# Kurczak ğŸ£

Minimal Ollama chat UI â€” no login, no heavy features. Pick a model and chat. Built for coding with markdown and syntax highlighting.

<img width="906" height="436" alt="image" src="https://github.com/user-attachments/assets/6c634167-8e6c-4a1a-9996-05f35747a2b2" />

## ğŸ Features

- **Model switcher** â€” Lists models from your Ollama instance
- **Streaming** â€” Responses appear token-by-token
- **Streaming continuity** â€” You can switch threads mid-generation; when you return, the in-progress assistant message continues updating live
- **Markdown & syntax highlighting** â€” Code blocks with language tags (e.g. ` ```javascript `)
- **Copy button** â€” On every code block; shows "Copied!" on green background
- **System prompt** â€” Optional (default in config), great for "you are a coding assistant"
- **Thinking view** â€” If the model emits thinking/reasoning, itâ€™s available in a collapsible â€œThinkingâ€ section (with a short preview)
- **Stop generation** â€” Abort an in-progress response
- **Message metadata** â€” Shows timestamp, model name, and generation duration (e.g. `25.7s`)
- **Context estimate** â€” Top-right badge with an approximate token count
- **History** â€” Stored as JSON files under `data/history/`; list, open, delete (no database)
- **Config** â€” Set Ollama URL and default system prompt in `config.json`

## âš™ï¸ Setup

1. Install dependencies:
   ```bash
   npm install
   ```

2. Edit `config.json` if needed:
   - `ollamaUrl`: your Ollama API URL (default `http://localhost:11434`)
   - `port`: server port (default `1234`)
   - `defaultModel`: model name to select by default (optional)
   - `defaultSystemPrompt`: pre-filled system prompt (e.g. for coding); use "System prompt" next to Send to show/edit

3. Run:
   ```bash
   npm start
   ```
   Dev with auto-restart: `npm run dev`

4. Open `http://localhost:1234` (or your VPS host/port).

## ğŸ”Œ Requirements

- Node.js 18+
- Ollama running and reachable at the URL in `config.json`

## ğŸ“‚ Project layout

```
kurczak/
  config.json     # Ollama URL, port, default system prompt
  server.js       # Express: proxy to Ollama, history API, static files
  data/history/   # One JSON file per conversation (created at runtime)
  public/
    index.html
    style.css
    app.js
```

History is saved automatically after each assistant reply. No DB or login â€” just files on disk.

## ğŸ’¡ Model switching and context

**Can you switch models in the middle of a conversation?** Yes. Change the model in the sidebar and send the next message; that message (and all previous ones) are sent to the newly selected model.

**Does the next model see the earlier conversation?** Yes. The app sends the conversation history with every request so the model has context. Each message is stored with an optional date and model name so you can see who (which model) said what.

### Avoiding context length limits

Ollama (and the model) have a finite context window (e.g. 4kâ€“128k tokens depending on model and `num_ctx`). Sending the whole conversation every time can hit that limit.

**Is sending the whole conversation the only way?** With Ollamaâ€™s stateless API, the only way to give the model â€œmemoryâ€ is to send messages in the request. You can reduce how much we send:

- **`maxMessagesInContext`** in `config.json`: set to a positive number (e.g. `20` or `50`). Only the **last N messages** of the current chat are sent to the model (system prompt is always included). The full thread is still stored in history and in the UI; only the API request is trimmed. Use this to avoid exhausting the context window on long chats.

The context badge is an estimate meant for quick feedback. It starts at `~0` for a brand-new thread, and begins counting the system prompt once youâ€™ve sent at least one message in that thread.

**What happens when the model hits the context limit?** Ollama may return an error (e.g. in the response body or as an NDJSON line with `error`). The app:

- On non-OK HTTP: reads the error body from the server and shows it in the chat (so you see Ollamaâ€™s message, e.g. context-related).
- During streaming: if a chunk contains `error`, it is shown as â€œError from model: â€¦â€ and the stream stops.

So you get a visible error in the chat when the backend reports a problem (including context limit). If you see that, try starting a new chat or setting `maxMessagesInContext` so the next request sends fewer messages.
